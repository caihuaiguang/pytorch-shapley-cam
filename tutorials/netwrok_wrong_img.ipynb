{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/caihuaiguang/miniconda3/envs/cfr/lib/python38.zip', '/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8', '/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/lib-dynload', '', '/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages', '/home/caihuaiguang/PIDCFR', '/home/caihuaiguang/PIDCFR/third_party/PokerRL', '/home/caihuaiguang/DSG/pytorch-shapley-cam']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/caihuaiguang/DSG/pytorch-shapley-cam')\n",
    "\n",
    "# 验证路径是否已添加\n",
    "print(sys.path)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  \n",
    "\n",
    "import torch\n",
    "# 设置设备为GPU或CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import timm\n",
    "import cv2\n",
    "import requests\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.metrics.road import ROADMostRelevantFirst\n",
    "from pytorch_grad_cam.metrics.road import ROADLeastRelevantFirstAverage, ROADMostRelevantFirstAverage\n",
    "from pytorch_grad_cam.metrics.road import ROADCombined\n",
    "from pytorch_grad_cam.metrics.cam_mult_image import CamMultImageConfidenceChange\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget, ClassifierOutputLnSoftmaxTarget, ClassifierOutputExclusiveLnSoftmaxTarget, ClassifierOutputReST, ClassifierOutputReST_2\n",
    "from pytorch_grad_cam.metrics.ADCC import ADCC\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM, EigenGradCAM, LayerCAM, AblationCAM, RandomCAM, ShapleyCAM, ShapleyCAM_x, ShapleyCAM_mean, ShapleyCAM_hires, GradCAMElementWise, HiResCAM, XGradCAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selet the layer before adaptive avg pooling, i.e., norm\n",
    "# def reshape_transform(result, height=7, width=7):\n",
    "#     # print(result.shape)\n",
    "#     result = result.transpose(2, 3).transpose(1, 2)\n",
    "#     return result\n",
    "\n",
    "# model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "# # target_layers = [model.norm]\n",
    "# target_layers = [model.layers[-1].blocks[-1].norm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reshape_transform(result, height=7, width=7):\n",
    "#     # print(result.shape)\n",
    "#     result = result.reshape(result.size(0),\n",
    "#                             height, width, result.size(2))\n",
    "\n",
    "#     # Bring the channels to the first dimension,\n",
    "#     # like in CNNs.\n",
    "#     result = result.transpose(2, 3).transpose(1, 2)\n",
    "#     return result\n",
    "\n",
    "# model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "# target_layers = [model.layers[-1].blocks[-1].norm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vit performance is not good\n",
    "# import torch\n",
    "# def reshape_transform(tensor, height=14, width=14):\n",
    "#     # print(tensor.shape)\n",
    "#     result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "#                                       height, width, tensor.size(2))\n",
    "\n",
    "#     # Bring the channels to the first dimension,\n",
    "#     # like in CNNs.\n",
    "#     result = result.transpose(2, 3).transpose(1, 2)\n",
    "#     return result\n",
    "# model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "\n",
    "# # target_layers =  [model.blocks[-1].norm1]\n",
    "# target_layers =  [model.blocks[-1].norm2]\n",
    "# # target_layers =  [model.norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = models.resnet152(pretrained=True)\n",
    "# reshape_transform = None\n",
    "# # target_layers = [model.layer4[-1].conv3]\n",
    "# target_layers = [model.layer4[-1].relu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = models.vgg16(pretrained=True)\n",
    "# reshape_transform = None\n",
    "# target_layers = [model.features[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = models.efficientnet_b0(pretrained=True)\n",
    "# reshape_transform = None\n",
    "# target_layers = [model.features[8][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "reshape_transform = None\n",
    "# target_layers = [model.layer4[-1].conv2]\n",
    "target_layers = [model.layer4[-1].relu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "# reshape_transform = None\n",
    "# target_layers = [model.features[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Showing the metrics on top of the CAM : \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_score\u001b[39m(visualization, name, adcc, avg_drop, coherency, complexity, IC, ADD ):\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/media/caihuaiguang/miniconda3/envs/cfr/lib/python3.8/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "# Showing the metrics on top of the CAM : \n",
    "def visualize_score(visualization, name, adcc, avg_drop, coherency, complexity, IC, ADD ):\n",
    "    visualization = cv2.putText(visualization, name, (10, 20), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, f\"ADCC: {adcc:.5f}\", (10, 40), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, f\"AD: {avg_drop:.5f}\", (10, 55), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    visualization = cv2.putText(visualization, f\"Coherency: {coherency:.5f}\", (10, 70), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA) \n",
    "    visualization = cv2.putText(visualization, f\"Complexity: {complexity:.5f}\", (10, 85), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    visualization = cv2.putText(visualization, f\"IC: {IC:.5f}\", (10, 100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA) \n",
    "    visualization = cv2.putText(visualization, f\"ADD: {ADD:.5f}\", (10, 115), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    return visualization\n",
    "    \n",
    "def benchmark(_img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category=None):\n",
    "    methods = [(\"GradCAM\", GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"HiResCAM\", HiResCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"LayerCAM\", LayerCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"GradCAM-E\", GradCAMElementWise(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #         #    (\"ScoreCAM\", ScoreCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"XGradCAM\", XGradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"GradCAM++\", GradCAMPlusPlus(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"ShapleyCAM-E\", ShapleyCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"ShapleyCAM-M\", ShapleyCAM_mean(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"ShapleyCAM-H\", ShapleyCAM_hires(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"ShapleyCAM-X\", ShapleyCAM_x(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #            (\"EigenGradCAM\", EigenGradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "    #         #    (\"AblationCAM\", AblationCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, ablation_layer=AblationLayerVit())),\n",
    "    #            (\"RandomCAM\", RandomCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)),\n",
    "               ]\n",
    "\n",
    "    cam_metric = ADCC()\n",
    "    # targets = [ClassifierOutputTarget(category)]\n",
    "    # targets = [ClassifierOutputSoftmaxTarget(category)]\n",
    "    # targets = [ClassifierOutputLnSoftmaxTarget(category)]\n",
    "    # targets = [ClassifierOutputExclusiveLnSoftmaxTarget(category)]\n",
    "    # targets = [ClassifierOutputEntropy(category)]\n",
    "    targets = [ClassifierOutputReST_2(category[0]),ClassifierOutputReST_2(category[1])]\n",
    "    metric_targets = [ClassifierOutputSoftmaxTarget(category)]\n",
    "    \n",
    "    visualizations = []\n",
    "    for name, cam_method in methods:\n",
    "        attributions = cam_method(input_tensor=input_tensor, \n",
    "                                    targets=targets, eigen_smooth=eigen_smooth, aug_smooth=aug_smooth)\n",
    "        attribution = attributions[0, :]   \n",
    "        adcc, avg_drop, coherency, complexity, inc, dropindeletion   =cam_metric(input_tensor,attributions,targets,metric_targets,model, cam_method)\n",
    "        visualization = show_cam_on_image(_img_example, attribution, use_rgb=True)\n",
    "        visualization = visualize_score(visualization, name, adcc[0], avg_drop[0], coherency[0], complexity[0], inc[0], dropindeletion[0] )\n",
    "        visualizations.append(visualization)\n",
    "    return Image.fromarray(np.hstack(visualizations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 标准的ImageNet图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 加载ImageNet验证集\n",
    "val_dataset = datasets.ImageFolder('/media/caihuaiguang/data/ILSVRC2012_img_val', transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 初始化统计变量\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 迭代验证集进行预测\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, labels) in enumerate(tqdm(val_loader, desc=\"Processing images\", unit=\"batch\")):\n",
    "        if batch_idx == 100:\n",
    "            break\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 模型推理\n",
    "        outputs = model(imgs)\n",
    "        # prob, predicted = torch.max(outputs.data, 1)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        prob, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "        # 统计准确数量\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # 查找错误预测的文件名\n",
    "        batch_start_idx = batch_idx * val_loader.batch_size  # 当前批次在数据集中起始的索引\n",
    "        for i in range(len(predicted)):\n",
    "            # if predicted[i] != labels[i]:  # 预测错误时\n",
    "            #     # 获取当前样本的文件路径\n",
    "            #     file_path = val_dataset.samples[batch_start_idx + i][0]\n",
    "            #     print(f\"Misclassified: {file_path}, Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "            epsilon = 1e-15  # 可以根据需要调整\n",
    "            if abs(prob[i] - 1) < epsilon:\n",
    "                file_path = val_dataset.samples[batch_start_idx + i][0]\n",
    "                print(f\"wow!: {file_path}, Predicted: {predicted[i].item()}, Actual: {labels[i].item()},  Prob: {prob[i]:.8f}\")\n",
    "\n",
    "# 计算并打印准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the ImageNet validation set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "# using this link to get label name from ID (start from 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mul\n",
    "# img_example = np.array(Image.open(\"./ILSVRC2012_val_00023935.JPEG\")) \n",
    "# img_example = np.array(Image.open(\"./ILSVRC2012_val_00007197.JPEG\")) \n",
    "# img_example = np.array(Image.open(\"./ILSVRC2012_val_00006149.JPEG\")) \n",
    "img_example = np.array(Image.open(\"./ILSVRC2012_val_00022113.JPEG\")) \n",
    "# img_example = np.array(Image.open(\"../examples/both.png\")) \n",
    "\n",
    "\n",
    "img_example = np.array(Image.fromarray(img_example).resize((224, 224)))\n",
    "img_example = np.float32(img_example) / 255\n",
    "input_tensor = preprocess_image(img_example, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "model.cuda()\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "model.eval()\n",
    "outputs = model(input_tensor)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "_, predicted_min = torch.min(outputs.data, 1)\n",
    "print(predicted) # 2: great white shark\n",
    "print(predicted_min) # 90: lorikeet\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.softmax(outputs,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取前5个最高概率的标签及其对应的索引\n",
    "top5_probs, top5_indices = torch.topk(outputs, 5, dim=1)\n",
    "\n",
    "# 获取前5个最低概率的标签及其对应的索引\n",
    "bottom5_probs, bottom5_indices = torch.topk(outputs, 5, dim=1, largest=False)\n",
    "\n",
    "# 打印前5个最高概率的标签和对应概率\n",
    "print(\"Top 5 labels and probabilities for each example:\")\n",
    "for i in range(outputs.size(0)): \n",
    "    for j in range(5):\n",
    "        print(f\"Label: {top5_indices[i][j].item()}, Probability: {top5_probs[i][j].item()}\")\n",
    "\n",
    "# 打印前5个最低概率的标签和对应概率\n",
    "print(\"\\nLowest 5 labels and probabilities for each example:\")\n",
    "for i in range(outputs.size(0)): \n",
    "    for j in range(5):\n",
    "        print(f\"Label: {bottom5_indices[i][j].item()}, Probability: {bottom5_probs[i][j].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category = top5_indices[0][0] #  label is 3 (tiger shark), but resnet18 think it is 2 (great white shark)\n",
    "device = outputs.device  # Make sure this matches the device where you are working\n",
    "# Create the batch of tensors correctly\n",
    "input_batch = torch.stack([input_tensor[0], input_tensor[0]]).to(device)  # Ensure proper batching\n",
    "# Create a tensor of categories for the batch\n",
    "category_batch = [category, category]\n",
    "print(category_batch)\n",
    "# Call the benchmark function\n",
    "benchmark(img_example, input_batch, target_layers, eigen_smooth=False, aug_smooth=False, category=category_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = top5_indices[0][1] #  label is 3(tiger shark), but resnet18 think it is 2(great white shark)\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = top5_indices[0][2] #  label is 3, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = top5_indices[0][3] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = top5_indices[0][4] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = bottom5_indices[0][0] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = bottom5_indices[0][1] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category =  bottom5_indices[0][2] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category =  bottom5_indices[0][3] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category =  bottom5_indices[0][4] #  label is 0, but resnet18 think it is 2\n",
    "benchmark(img_example, input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category= category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
